{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc7486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284e2e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\torchaudio\\backend\\utils.py:63: UserWarning: The interface of \"soundfile\" backend is planned to change in 0.8.0 to match that of \"sox_io\" backend and the current interface will be removed in 0.9.0. To use the new interface, do `torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False` before setting the backend to \"soundfile\". Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002f476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52209d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cdfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d4bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the dataset\n",
    "datapath = Path('Lesson1dataset/ESC-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc79811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16acaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(datapath / Path('meta/esc50.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c8ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target        category  esc10  src_file take\n",
       "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
       "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
       "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
       "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
       "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3a10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, sr = torchaudio.load(datapath / 'audio' / csv.iloc[0,0], normalize=True)\n",
    "x, sr = torchaudio.load(datapath / 'audio' / csv.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50b1b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 220500])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7a6d0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#useful to take the audio and reduce the sampling rate\n",
    "torchaudio.transforms.Resample(orig_freq=sr, new_freq=8000)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add4058e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\torchaudio\\functional.py:317: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\torch\\functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  ..\\aten\\src\\ATen\\native\\SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\torch\\functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  ..\\aten\\src\\ATen\\native\\SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "h = torchaudio.transforms.MelSpectrogram(sample_rate=sr)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae76e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 1103])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#signal processing, now we have a sort of image (small chunck): width=time height= frequency content\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d7a0ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a376e34d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "869b9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to logarithm, easier to work with NN\n",
    "h = torchaudio.transforms.AmplitudeToDB()(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92aea1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a37df5e490>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAABKCAYAAABAUxQ5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPQUlEQVR4nO3dW6xc113H8e9vrb1nzplzju24aXNtm1RELYEHelFpKUIVAVEKorwgpaiiSEV9AYnLA0rVB8RDJUAIIYRAitpCubWqSgWoKuISkBASKgn3tGnapAmN4ySO8e1cZmbvvdafh7VtnyaO7Tg+Z8bb/480mj1rbvs/nvP3mv9eey2ZGc4554YlLHoHnHPOXX2e3J1zboA8uTvn3AB5cnfOuQHy5O6ccwPkyd055wZoz5K7pPdIelTSY5Lu26v3cc4592Lai3HukiLwNeAHgSPAg8D7zewrV/3NnHPOvche9dzfDjxmZt8wswb4DPC+PXov55xzL7BXyf024Kldt4/0bc455/ZBtUevqwu0fUv9R9KHgQ8DROJbJxzYo11xzrlh2uTkcTN79YXu26vkfgR47a7btwNHdz/AzO4H7gc4oMP23bpnj3bFXY+q225l5ztvZfaqilQLi5BrCC2kMWAQOohzCJ2d646E1si1wKCaGe1E1FOjGwtliK2x9sQW4cmjpJMnFxqjc39vn/vfl7pvr5L7g8Bdku4EngbuBX5yj97LuRfJZzZZffIU42NjrA5YHTGBDCwIZQMzlA2lfrtNWB0hGTKDnLE6ojaBVB6TDJ3eIk+niw7RuYvak+RuZp2knwP+BojAJ83sy3vxXs5diEY13Y3rtBs1aUWkUX94yQyLKr11Sq9dCZSN0BppFEpbl1GGNA7EeT7fs+8yddtBXcNstqDonLu0veq5Y2ZfBL64V6/v3EVVFc2hmnYt0qyLXJVeO7lk6dAJBErl4SEZymB9Eq9mAZmRapFWSsLPtVAy6uMVGtWLicu5y7Rnyd25RVIIzA9Gzrw+EBKkEeSREealdh4SWCg1eCVQKr350IJFaDZEbOiTOoCwAMoijyo/tdstPU/ubphiJNWiWze6VUNJ5JUMgNWGOqG2T/SNCC2Euaj6UrqFcrC11NohNqV8E+dAFVDtPXe33Dy5u2EyIzZG3Ak0N7eoylgKKBihyuQ2YE0AExYCuRJhVOrxstKbz3Wf5FtKScYgrUBzcMTq8ZVFR+jcRXlyd8OUc19GgTBKrK41NPOKbCLGjFWJLlRYClijksyjsPW+9g5UO2UIpTJlWABQp/62LnQqh3PLw5O7G6aVMWfuCIS3nuZdtzzFm9aeo7VIrcQkzmktspVW2OrGnGonPL1zkFlX0+XAmdmYKGPW1AQZTVORk7As5sfGrD1bM/6mJ3e33Dy5u2HqEpNnjRNH1/nnM3fx0PrrqKvEZNwQVIZBZhNNF2lTZD6vMYN2WsM8okYoCzLEuYgqJZpqS1TTVMbJO7fEPLm7QbL1VXZuEfHwnDe/7im+be15DlZTJqHhcLVF6sc8Pj6/iUloSBY4Oj/Esfk6J+cTdtqaaVMzbytyDrRtJLeB7obI9tGK1SNrC47QuYvz5O6GSaJbM1ZWS099HDqCMoerLdbCnBW1HOs2ADjertNa5FS7SqVMHRIrVUn+KQdmXcSSyiD4JEKHL3Pjlp4ndzdMZoRGVCHzxvXnuHN8jNdUm0zCnDU1JMSKWgCeaW9gM62wPpkzTWWI4yxVrNWRjdGczWZM01U0XWR7ewULkTBtSYuMz7lL8OTuhillZLC9M+aRzZuplWitIipza3WSqExr5et/tuc+TTWn21XONCucmE7IJtouMm8rUgqEYKSdqoyWabvFxufcJfiPSzdMVaTvmHN6vspmWiERWFHLjo0BqHU+QU9TzXY3ZhQ6RiERQ6ZLgRAyMWZSF+naCFbOdrXV0SKicu6yec/dDZKNKmQQq8zB8ZSb6jO8Km5RqyMqEzESYmYlSZ9uV+lyoMuRJkcEmIkuB4KMEDKpC5DLtASaNosN0LlL8OTuBkltIs6hqjuaVHE6rfJsd5Cn5zecGzUzDi1Pzm7kuXk5sPp/szXMhGSc2JqQs+jaitwJa8pZTJoHqpmhzivubrl5cnfDI5EnIzDY2lphe2PE0dkhZrmmVmIcWlqL7HQjttKYE/M1JlXDdjNipeqYNiO6rp+aIINlQVdGz1Q7opp5YnfLz5O7Gx4zwvYcpQ3yTsVWMyIo8/T0EAfqMgf7RpxxrNngVLsKwLPbB5i1VRnbPqtJKZD73jrzQJiGUuaZlSRvVVxIaM5dLk/ubpDyuC7L6J2uOL6+wSPhZuqYmI0rjs/XuGG0w2a7won5hK1mzPZ8RNtWdG2k264hGMzLeIO4HZH10wN3UG0ln1vGLT1P7m54ziZeK9P7WhZBRjax040YhcTzs3W22zFNjlQhc3B1RpcDKQnVGesTu3KZVMwqw0I5gUkZ5EMh3ZLzoZBueMxQSiCwaGDQ5sAoJFIOdBbY6UZstyNObE/ocqDNgfmsJqeIqgwBZOfnfI9znZv33aqynqpzy8x77m54JKyOhNaoNgPtuOL0ziqnbJUQjLVxQwyZWVumAD6xNaFtKlIbsGmFWhFnoUwzYOUgqnLZLkv1GVb7n45bbt5zd4NkMYCg3hRqysRfAKOqI5tIORAE6ytzYsxUdenpU5XFsK22su5qorT3HfVcQR4HlPKiQnPusnhyd8MUhAmqKagTXRsJwTATk7pltW4ZVR0pB3IWKaksnq1SXz9LuaydisoldEAGkg+HdMvNf1u6QVKyc8vljU4FZuuRsF6S+05bk3I56Np0VSnJpIAloWlEXV9fn9PXYc6uowphjs/l7q4Jl+y5S/qkpGOSHt7VdljS30n6en99w677PiLpMUmPSvqhvdpx5y7GYum557rM1BvGiY2VOZNxw83rm9x24Aw3rW9xYHXGeKUlVqmMkllJ5HEmj4xc92WYqmynUf96UT4U0i29yynL/CHwnhe03Qc8YGZ3AQ/0t5F0N3Av8B39c35Pkp/t4fadhb7EEmH+mo61jRmrdct63TAKHZOqzPM+ionVUct43FGNOlRnCFaGUMay+pLOdtT70owyfkDVLb1LJncz+yfgxAua3wd8qt/+FPDju9o/Y2ZzM3sCeAx4+9XZVedeBiuXXAHR2FiZE5VZq+cANCl+S4KvQn+AtF+hiQSEUto5V3OnbOdaEPxwlVtuV/oNvcnMngHor1/Tt98GPLXrcUf6theR9GFJD0l6qGV+hbvh3EUI0gpQZ1IOJAt0FulyJMjKSUs50KSyjqrlgLUBdYEwD6g9O7ZdqCv1+7PDI51bdlf7t+WFCpEX/FMws/uB+wEO6LD/ubirKwiL5aQjppFpU1ZY2pyPiSET+1rLdlOzMxvTtpG0XaNZIE4D6so8MmcTuvpfAhbKAVU17eJic+4yXGlyf07SLWb2jKRbgGN9+xHgtbsedztw9JXsoHNXwgTkMsJldCKyOZkwW20Zj1vqmMg5nFtpqZlVZZKwToRZOHdG6rnEfnZIu5XbFuQTh7mld6Vlmb8CPthvfxD4y13t90oaS7oTuAv411e2i85dubMldMvCspjNauZtzXRes7MzZrY1JrcR2kDoz0qN09LjD+l8j/3cSUwRZAae3N2Su2TPXdKngXcDN0o6AvwK8GvAZyV9CPgm8BMAZvZlSZ8FvgJ0wM+amZ/t4fbd2bHodrb70gS6UGGdSOOE9ePaaQNKIsxEmIvYlB57eY3+P4eqXMvAMqRRAB/r7pbcJZO7mb3/Je665yUe/zHgY69kp5x7pawOWHX+HCQlYTuxnHHaXxMMIlSbgTgV1ZRzpZxz88hQeusAeVReK43kQyHd0vNvqBskUzmJqd6CHMXGNwKT44mdG0tNffJ8QmbkWoR5R0hWJgSLQskg6Ft651YF5oci7aqod3KZddK5JebJ3Q1SaDMhQWiNG76WqWbG6TsqzrwpsXokEpvAyW8X9ZZYO2pMjnVlFEw/bQGdITNM5WSoHGF6ONBulFWYVp8ZLzpE5y7Kk7sbJivzuKuD8ckOGRx8EtaeFaPNFswYPRgIbUnoobPzdXX1E0GqzCljUaRxQGaMzkC97fV2t/xkS7DogKRN4NFF78c+uxE4vuid2Gce8/Bdb/HCYmN+vZm9+kJ3LEvP/VEze9uid2I/SXrIYx6+6y3m6y1eWN6YfYIM55wbIE/uzjk3QMuS3O9f9A4sgMd8fbjeYr7e4oUljXkpDqg655y7upal5+6cc+4qWnhyl/Sefkm+xyTdt+j9uRokvVbSP0p6RNKXJf183z745QklRUn/IekL/e1BxyzpkKTPSfpq/+/9ziHHLOkX++/0w5I+LWllaPFeraVFJb1V0v/09/2OtM9rM5rZwi5ABB4H3gCMgP8C7l7kPl2luG4B3tJvbwBfA+4GfgO4r2+/D/j1fvvuPvYxcGf/mcRFx3GFsf8S8GfAF/rbg46ZshLZz/TbI+DQUGOmLLzzBLDa3/4s8NNDixf4PuAtwMO72l52jJQZcd9JOSfur4Ef3s84Ft1zfzvwmJl9w8wa4DOUpfquaWb2jJn9e7+9CTxC+cMY9PKEkm4HfgT4+K7mwcYs6QAlEXwCwMwaMzvFgGOmnBuzKqkCJpT1GgYVr12FpUX7dS4OmNm/WMn0f7TrOfti0cn9spflu1ZJugN4M/AlrsLyhEvut4FfBvKutiHH/AbgeeAP+lLUxyWtMdCYzexp4Dcp03w/A5w2s79loPG+wMuN8bZ++4Xt+2bRyf2yl+W7FklaB/4c+AUzO3Oxh16g7Zr6HCT9KHDMzP7tcp9ygbZrKmZKL/YtwO+b2ZuBbcpP9pdyTcfc15nfRyk/3AqsSfrAxZ5ygbZrJt7L9FIxLjz2RSf3wS7LJ6mmJPY/NbPP983P9T/XGODyhO8CfkzSk5Ty2vdL+hOGHfMR4IiZfam//TlKsh9qzD8APGFmz5tZC3we+B6GG+9uLzfGI/32C9v3zaKT+4PAXZLulDQC7qUs1XdN64+KfwJ4xMx+a9ddg12e0Mw+Yma3m9kdlH/HfzCzDzDsmJ8FnpL0xr7pHsoqZEON+ZvAOyRN+u/4PZTjSUONd7eXFWNfutmU9I7+s/qpXc/ZH0twZPq9lNEkjwMfXfT+XKWYvpfyE+y/gf/sL+8FXgU8AHy9vz686zkf7T+DR9nno+p7EP+7OT9aZtAxA98FPNT/W/8FcMOQYwZ+Ffgq8DDwx5RRIoOKF/g05ZhCS+mBf+hKYgTe1n9OjwO/S3/S6H5d/AxV55wboEWXZZxzzu0BT+7OOTdAntydc26APLk759wAeXJ3zrkB8uTunHMD5MndOecGyJO7c84N0P8DasDwhpp+agQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9eb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1340dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0479b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC50Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path: Path = Path('Lesson1dataset/ESC-50'),\n",
    "                sample_rate:int = 8000,\n",
    "                folds = [0]): \n",
    "        #path=root folder of the dataset (with : you specify type and default value, folds see csv: we are telling which folds we want to import, by default only the first one)\n",
    "        #Load CSV & Initialize all torchaudio.transforms (standard ops when working with audio)\n",
    "        #Resample --> MelSpectrogram ->AmplitudeToDB \n",
    "        self.path = path\n",
    "        self.csv = pd.read_csv(path / Path('meta/esc50.csv'))\n",
    "        self.csv = self.csv[self.csv['fold'].isin(folds)]\n",
    "        self.resample = torchaudio.transforms.Resample(\n",
    "            orig_freq=44100, new_freq=sample_rate\n",
    "        )\n",
    "        self.melspec=torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)#n_mels input changes the height of the img\n",
    "        self.db = torchaudio.transforms.AmplitudeToDB()#top_DB other nice parameter\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        #index using square brackets; like numpy, Pythorch indexes\n",
    "        #Return (xb, yb) pair (xb is spectogram of the index i, and the second one is the class (an integer))\n",
    "        row = self.csv.iloc[index]\n",
    "        wav, _ = torchaudio.load(self.path / 'audio' / row['filename']) #or row.iloc[0]\n",
    "        label=row['target']\n",
    "        xb = self.db(\n",
    "            self.melspec(\n",
    "                self.resample(wav)\n",
    "            )\n",
    "        )\n",
    "        return xb, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        #tells pytorch how many objects you have in this dataset\n",
    "        #Returns length\n",
    "        return len(self.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4f6d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ESC50Dataset()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62b04e6e",
   "metadata": {},
   "source": [
    "for xb,yb in train_data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e4d1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  ESC50Dataset(folds=[1])\n",
    "valid_data =  ESC50Dataset(folds=[2])\n",
    "test_data =  ESC50Dataset(folds=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45722137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)#small batches without GpU, and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d4386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(valid_data, batch_size=8, shuffle=True)#small batches without GpU, and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81c9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True)#small batches without GpU, and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "800ffb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2 building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "550d32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 channel in input, \n",
    "class AudioNet(torch.nn.Module):\n",
    " \n",
    "    def __init__(self, n_classes = 50, base_filters = 64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, base_filters, 11, padding=5)\n",
    "        self.bn1 = nn.BatchNorm2d(base_filters)\n",
    "        self.conv2 = nn.Conv2d(base_filters, base_filters, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(base_filters, base_filters * 2, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(base_filters * 2)\n",
    "        self.conv4 = nn.Conv2d(base_filters * 2, base_filters * 4, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(base_filters * 4)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(base_filters * 4, n_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.fc1(x[:, :, 0, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07a7b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "audionet = AudioNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d23dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(train_loader)) # a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a740937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 128, 201])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "445f435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audionet(xb).shape #for each audio file, a prediction over 50 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a6136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a3e329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch does not have nice high-level utilities:\n",
    "#I.e., to train the model, write everything step by step (also save checkpoints, conf parameters savings)\n",
    "#For this reason they started used pytorch lightening library, similar to keras, very simple\n",
    "#e.g. use LitAutoEncoder and left everything everything to it\n",
    "#https://pytorch-lightning.readthedocs.io/en/latest/starter/new-project.html#step-1-define-lightningmodule\n",
    "#to change, simply edit the edit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11fc62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 channel in input, \n",
    "class AudioNet(pl.LightningModule):\n",
    " \n",
    "    def __init__(self, n_classes = 50, base_filters = 64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, base_filters, 11, padding=5)\n",
    "        self.bn1 = nn.BatchNorm2d(base_filters)\n",
    "        self.conv2 = nn.Conv2d(base_filters, base_filters, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(base_filters)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(base_filters, base_filters * 2, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(base_filters * 2)\n",
    "        self.conv4 = nn.Conv2d(base_filters * 2, base_filters * 4, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(base_filters * 4)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(base_filters * 4, n_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.fc1(x[:, :, 0, 0])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x, y = batch     \n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y) #includes a softma self.fc1(x[:, :, 0, 0]) inside for computational reasons\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx): #prima validations steps non c'erano, e credo che il training fini dopo un'epoca\n",
    "        #for validation (in classification). So, instead of logging loss, we log accuracy\n",
    "        x,y = batch\n",
    "        y_hat = self(x)\n",
    "        #don't put everything into eval or training; it's done automatically\n",
    "        y_hat = torch.argmax(y_hat, dim=1)#taking predictions and \n",
    "        acc = p1.metrics.functional.accuracy(y_hat, y)\n",
    "        self.log('val_acc', acc, on_epoch=True, progr_bar=True)#log also on the progress bar\n",
    "        return acc\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad17f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audionet = AudioNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acae1835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0) #pytorch has this function it sets a seed 0, important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277db8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e2104f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer=pl.Trainer(gpus=1, max_epochs=1) #which gpus? (it's not tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f96bfef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name  | Type        | Params\n",
      "---------------------------------------\n",
      "0  | conv1 | Conv2d      | 7.8 K \n",
      "1  | bn1   | BatchNorm2d | 128   \n",
      "2  | conv2 | Conv2d      | 36.9 K\n",
      "3  | bn2   | BatchNorm2d | 128   \n",
      "4  | pool1 | MaxPool2d   | 0     \n",
      "5  | conv3 | Conv2d      | 73.9 K\n",
      "6  | bn3   | BatchNorm2d | 256   \n",
      "7  | conv4 | Conv2d      | 295 K \n",
      "8  | bn4   | BatchNorm2d | 512   \n",
      "9  | pool2 | MaxPool2d   | 0     \n",
      "10 | fc1   | Linear      | 12.8 K\n",
      "---------------------------------------\n",
      "427 K     Trainable params\n",
      "0         Non-trainable params\n",
      "427 K     Total params\n",
      "1.711     Total estimated model params size (MB)\n",
      "C:\\Users\\thefo\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:68: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for validation and test dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca17d55495544e098d7a4c09a56ac9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'p1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-3790b350b818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudionet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# double dispatch to initiate the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[1;31m# set stage for logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[1;34m(self, ref_model)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;31m# run eval step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_sanity_check_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[1;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[0;32m    724\u001b[0m                 \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluation_step_and_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;31m# capture any logged information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Algorand\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-81fa958ec844>\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#don't put everything into eval or training; it's done automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#taking predictions and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogr_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#log also on the progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p1' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(audionet, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b9607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15cb8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ecaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
